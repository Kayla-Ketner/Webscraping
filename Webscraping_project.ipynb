{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafd9efc-12f9-4612-b590-9ff5c8d24514",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as BS\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92365e05-bdac-4f39-b619-cf8366de277a",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'https://realpython.github.io/fake-jobs/'\n",
    "\n",
    "response=requests.get(URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bbcafb-196c-42fc-a7f5-9a81c9e1378b",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff40d22-32a1-4703-a108-034a70f49a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45110a95-bae9-471e-ae93-e2d01b913ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e446849-ed6b-4e53-b992-1cd009dc2f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup= BS(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb9bca4-f3dd-47a4-ae9d-a38a58fc1095",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515c24f3-1618-4793-bf88-7d09ebfe3b70",
   "metadata": {},
   "source": [
    "Use the .find method to find the tag containing the first job title (\"Senior Python Developer\"). Extract the text from this title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222ffbb1-f35f-4fa4-ba42-112b63b13838",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find('h2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac09d28-2683-463e-ba30-2ad8b7757b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find('h2').text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe50292-b450-45d9-9f58-f25cb76020ca",
   "metadata": {},
   "source": [
    "Now, use what you did for the first title, but extract the job title for all jobs on this page. Store the results in a list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fc61ed-b64c-47ae-9c99-26f53c3e3e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles=soup.find_all('h2', attrs={'class':'title'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963962e9-1c30-4d5b-9b05-6d515cb1b2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_list=[title.text for title in titles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980def64-5f96-455d-a424-c45a740ee87e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "titles_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e8511c-536f-4ae9-9a89-a674a3c6c37e",
   "metadata": {},
   "source": [
    "Extract the companies, locations, and posting dates for each job. Ensure that the text that you extract is clean, meaning no extra spaces or other characters at the beginning or end. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9926c07b-bb9a-41db-976b-ad43879911fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "companies=soup.find_all('h3', attrs={'class':'subtitle'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da07cf5-0f3d-4bb7-b398-c2e47a3952d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "companies_list=[company.text for company in companies]\n",
    "companies_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ab12d2-4a34-445e-bd23-2ba569d56c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "locations=soup.find_all('p', attrs={'class': 'location'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a755bc7d-5bc2-4e8c-a8c4-0212d38a909b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "locations_list=[location.text.strip() for location in locations]\n",
    "locations_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86865638-8080-44e9-9e03-e9ab49b32063",
   "metadata": {},
   "outputs": [],
   "source": [
    "times=soup.find_all('time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e245c6-4a65-4019-bda0-12fe91569ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "times_list=[time.text for time in times]\n",
    "times_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2130d72-af06-49ab-b735-001974e14b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dates=soup.find_all('p', attrs={'class': 'is-small'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e9cbc0-5ce5-4904-b3bd-671dbbe6f330",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dates_list=[date.text.strip() for date in dates]\n",
    "dates_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428f5b28-eb86-4662-8701-498b8aa6778c",
   "metadata": {},
   "source": [
    "Take the lists that you have created and combine them into a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0d31ab-d90d-4a39-9629-f5fdd771f8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_lists={'Job Title': titles_list, 'Company': companies_list, 'Location': locations_list, 'Date': dates_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdb2c44-c8e1-481e-bff9-3e5943dd3d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5f3e5a-eefa-47a5-abef-cfef9702d7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_table=pd.DataFrame(all_lists)\n",
    "jobs_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fafc595-fb73-4415-b658-041e609fa123",
   "metadata": {},
   "source": [
    "Next, add a column that contains the url for the \"Apply\" button. Try this in two ways.   \n",
    "    a. First, use the BeautifulSoup find_all method to extract the urls."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddda312-f7d0-41e9-a93b-a1a623caa46b",
   "metadata": {},
   "source": [
    "list=[link for, ind, link in enumerate(links) if ind %2 != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d3bd4b-6d15-4d8b-9e56-6241a2d90de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "links=[]\n",
    "for link in soup.find_all('a')[1::2]:\n",
    "    links.append(link.get('href')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d94facb-c026-44e3-88f5-043e1f989655",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458bd2b4-ac7e-41b3-b4bf-f1fca3408a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_table['Link']=links\n",
    "jobs_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f432e91-d330-4e28-9675-d8d59f22fe98",
   "metadata": {},
   "source": [
    "Next, get those same urls in a different way. Examine the urls and see if you can spot the pattern of how they are constructed. Then, build the url using the elements you have already extracted. Ensure that the urls that you created match those that you extracted using BeautifulSoup. Warning: You will need to do some string cleaning and prep in constructing the urls this way. For example, look carefully at the urls for the \"Software Engineer (Python)\" job and the \"Scientist, research (maths)\" job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a21c746-2bc8-43d4-a7b7-bcb60a5567eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_table['Link2']=jobs_table['Job Title']\n",
    "jobs_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8a7a95-cb8b-4aa2-b3dc-f297f29c383b",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_table['Link2']=jobs_table['Link2'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4017fffe-1cd1-4278-855b-f59565797702",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_table['Link2']=jobs_table['Link2'].str.replace('(','').str.replace(')','').str.replace('/',' ').str.replace(',','').str.replace(' ','-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0d2252-8c5e-4f7a-9185-1a11ab73bb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth', None)\n",
    "jobs_table['Link2']='https://realpython.github.io/fake-jobs/jobs/' + jobs_table['Link2']+ '-' \n",
    "jobs_table['Link2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af76d49c-6527-4765-9ec9-96f0f4fa91c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter=0\n",
    "link2_list=[]\n",
    "\n",
    "for link in jobs_table['Link2']:\n",
    "    link2_list.append(link + str(counter))\n",
    "    counter+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3ba6a3-98b5-43eb-ac62-5e0586e8a758",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "link2_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3def34-d250-4ae3-851a-6fe022ea978f",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_table['Link2']=link2_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb90ddf-ac4d-4f72-bb50-9769311b7282",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_table['Link2']=jobs_table['Link2']+ '.html'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d84da9-ff9c-4777-ba4b-df5395c58db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9de22dc-ca31-4b5c-abe3-6a5a2cf82497",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind, row in jobs_table.iterrows():\n",
    "    if row.Link != row.Link2:\n",
    "        print(ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542853e7-5a7b-4c3d-9cf7-2ab8e5c86250",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_table['Link'].equals(jobs_table['Link2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3bb5d1-6f2e-41b5-9505-77b5065a8031",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_table['Link']==jobs_table['Link2']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a1a903-1d1e-470c-a136-9c7aa3c0bf03",
   "metadata": {},
   "source": [
    "Finally, we want to get the job description text for each job.  \n",
    "    a. Start by looking at the page for the first job. Using BeautifulSoup, extract the job description paragraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a404536-52cc-4d8c-8ce8-c61a7bb6ee5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'https://realpython.github.io/fake-jobs/jobs/senior-python-developer-0.html'\n",
    "\n",
    "response2=requests.get(URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f929128a-33e0-40d5-9748-7e46a742f467",
   "metadata": {},
   "outputs": [],
   "source": [
    "response2.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31367c7b-3e45-4424-815e-50ce6ca2305f",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup2=BS(response2.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f592e7-d4e3-4cfb-b920-83a6bb31de96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(soup2.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6643ab47-e077-4d96-a7eb-3f06ae33297c",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup2.find_all('p')[1].text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7940f9-d3f1-426c-b4ef-24aca8afabcf",
   "metadata": {},
   "source": [
    "b. We want to be able to do this for all pages. Write a function which takes as input a url and returns the description text on that page. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831672a7-6c6d-47ad-8110-7d749c69df8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "requests.get('https://realpython.github.io/fake-jobs/jobs/senior-python-developer-0.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7408a352-d9d3-4c7b-bceb-7dab16ea1d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def job_des(url):\n",
    "            \n",
    "        def inner(url):\n",
    "            new_url=requests.get(url)\n",
    "            return BS(new_url.text)\n",
    "   \n",
    "        return inner(url).find_all('p')[1].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a133d39-f6bc-46de-8644-1ee485541f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_des('https://realpython.github.io/fake-jobs/jobs/senior-python-developer-0.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebbe303-4eeb-4965-bce9-6d2571640d1f",
   "metadata": {},
   "source": [
    "c. Use the [.apply method] on the url column you created above to retrieve the description text for all of the jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13efae9-8b3b-41ea-9816-30038530b1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_table['Link'].apply(job_des)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
